---
title: "AlgebraOfSummaries"
author: "Win-Vector LLC"
date: "March 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


From help("confusionMatrix") {caret}:

              Reference	
    Predicted	Event	No Event
         Event	A	    B
      No Event	C	    D

Reference is "ground truth" or actual outcome.  We will call examples that have true ground truth "true examples" (please don't confuse this with "TrueNegatives" which are "false examples" that are correctly scored as being false.

We can encode what we have written about these confusion matrix summaries as alebraic statements.

```{r algebrasetup}
library('rSymPy')

# From help("confusionMatrix") {caret}:
A = Var('TruePositives')
B = Var('FalsePositives')
C = Var('FalseNegatives')
D = Var('TrueNegatives')
# (A+C) and (B+D) are facts about the data, independent of classifier.
Sensitivity = A/(A+C)
Specificity = D/(B+D)
Prevalence = (A+C)/(A+B+C+D)
PPV = (Sensitivity * Prevalence)/((Sensitivity*Prevalence) + ((1-Specificity)*(1-Prevalence)))
NPV = (Specificity * (1-Prevalence))/(((1-Sensitivity)*Prevalence) + ((Specificity)*(1-Prevalence)))
DetectionRate = A/(A+B+C+D)
DetectionPrevalence = (A+B)/(A+B+C+D)
BalancedAccuracy = (Sensitivity+Specificity)/2


# From our slides
FNR = C/(A+C)
FPR = B/(B+D)
TPR = A/(A+C)
FPR = B/(B+D)
Recall = A/(A+C)
Precision = A/(A+B)
```



Obviously: Sensitivity==TPR==Recall.  But we can check much more.

```{r algebraderivations}
# Examine rules
print(FNR)

# Confirm TPR == 1 - FNR
sympy(paste("simplify(",TPR-(1-FNR),")"))

# Confirm Recall == Sensitivity
sympy(paste("simplify(",Recall-Sensitivity,")"))

# Confirm Precision != Specificity
sympy(paste("simplify(",Precision-Specificity,")"))

# Confirm PPV == Precision
sympy(paste("simplify(",PPV-Precision,")"))
```

Confirm Prob[score(true)>score(false)] (with half point on ties) == BalancedAccuracy FOR Hard Classifier.  Reference/Truth/Outcome "True" cases are A and C, "False" cases are B and D.  So the odds of drawing an ordered pair where the first is True and the second is False is: (A+C)*(B+D).  There are also four combinations of how a True example and a False example can be scored as a pair:

    A D : True Positive and True Negative: Correct sorting 1 point
    A B : True Positive and False Positive (same prediction "Positive", different outcomes): 1/2 point
    C D : False Negative and True Negative (same prediction "Negative", different outcomes): 1/2 point
    C B : False Negative and True Negative: Wrong order 0 points
    
The conditional expectation of Award[score(true)>score(false)] (1 point if score is right, 1/2 if tie, 0 if wrong) is then given by:
   
```{r balancedaccuracy}
ScoreTrueGTFalse = (1*A*D  + 0.5*A*B + 0.5*C*D + 0*C*B)/((A+C)*(B+D))
sympy(paste("simplify(",ScoreTrueGTFalse-BalancedAccuracy,")"))
```

Confirm  Prob[score(true)>score(false)] (with half point on ties) == AUC.  We can compute the AUC of the above confusion matrix by refering to the following diagram.

![ComputingArea](AUC.png)

Then we can check for general equality:

```{r auc}
AUC = (1/2)*FPR*TPR + (1/2)*(1-FPR)*(1-TPR) + (1-FPR)*TPR
sympy(paste("simplify(",ScoreTrueGTFalse-AUC,")"))
```



